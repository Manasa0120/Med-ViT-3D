{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Efficient Convolution Block-3D"
      ],
      "metadata": {
        "id": "FyimMFEaJIA2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Simple patchembdedding to run and test ECB. Can be modified according to architecture.\n",
        "class PatchEmbed3D(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1):\n",
        "        super().__init__()\n",
        "        self.proj = nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1)\n",
        "    def forward(self, x):\n",
        "        return self.proj(x)"
      ],
      "metadata": {
        "id": "3N-0FuQzLYpz"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZXYnMdrVAhlW"
      },
      "outputs": [],
      "source": [
        "#MHCA 3D\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from functools import partial\n",
        "from timm.models.layers import DropPath # importing DropPath\n",
        "\n",
        "NORM_EPS = 1e-5  # Use the same epsilon as original architecture\n",
        "\n",
        "class MHCA3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Multi-Head Convolutional Attention (3D version)\n",
        "    \"\"\"\n",
        "    def __init__(self, out_channels, head_dim):\n",
        "        super(MHCA3D, self).__init__()\n",
        "        norm_layer = partial(nn.BatchNorm3d, eps=NORM_EPS)\n",
        "        self.group_conv3x3 = nn.Conv3d(\n",
        "            out_channels, out_channels,\n",
        "            kernel_size=3, stride=1, padding=1,\n",
        "            groups=out_channels // head_dim, bias=False\n",
        "        )\n",
        "        self.norm = norm_layer(out_channels)\n",
        "        self.act = nn.ReLU(inplace=True)\n",
        "        self.projection = nn.Conv3d(\n",
        "            out_channels, out_channels, kernel_size=1, bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.group_conv3x3(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.act(out)\n",
        "        out = self.projection(out)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Local Feed forward Network alredy defined in LTB module\n",
        "\n",
        "class LocalFeedForward3D(nn.Module):\n",
        "    def __init__(self, in_channels, expand_ratio=4):\n",
        "        super(LocalFeedForward3D, self).__init__()\n",
        "        hidden_dim = in_channels * expand_ratio\n",
        "\n",
        "        self.conv1 = nn.Conv3d(in_channels, hidden_dim, kernel_size=1, bias=False)    # 1x1x1 conv\n",
        "        self.bn1 = nn.BatchNorm3d(hidden_dim)\n",
        "        self.relu1 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.dwconv = nn.Conv3d(hidden_dim, hidden_dim, kernel_size=3, stride=1, padding=1,\n",
        "                                groups=hidden_dim, bias=False)                        # 3x3x3 depthwise conv\n",
        "        self.bn2 = nn.BatchNorm3d(hidden_dim)\n",
        "        self.relu2 = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.conv2 = nn.Conv3d(hidden_dim, in_channels, kernel_size=1, bias=False)    # 1x1x1 conv\n",
        "        self.bn3 = nn.BatchNorm3d(in_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu1(x)\n",
        "\n",
        "        x = self.dwconv(x)\n",
        "        x = self.bn2(x)\n",
        "        x = self.relu2(x)\n",
        "\n",
        "        x = self.conv2(x)\n",
        "        x = self.bn3(x)\n",
        "\n",
        "        return identity + x   # Residual connection"
      ],
      "metadata": {
        "id": "IuW1_1sFKx8R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calling ECB class\n",
        "\n",
        "\n",
        "class ECB3D(nn.Module):\n",
        "    \"\"\"\n",
        "    Efficient Convolution Block (3D)\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, path_dropout=0,\n",
        "                 drop=0, head_dim=32, mlp_ratio=3):\n",
        "        super(ECB3D, self).__init__()\n",
        "        self.in_channels = in_channels\n",
        "        self.out_channels = out_channels\n",
        "        norm_layer = partial(nn.BatchNorm3d, eps=NORM_EPS)\n",
        "        assert out_channels % head_dim == 0\n",
        "\n",
        "        self.patch_embed = PatchEmbed3D(in_channels, out_channels, stride)  # <--  Patch embedding should be 3D before it is fed to ECB-3D. This uses a simple patch embedding (not from original architecture to make the function work. You can improvise using base architecutre)\n",
        "        self.mhca = MHCA3D(out_channels, head_dim)                         # <-- our 3D MHCA\n",
        "        self.attention_path_dropout = DropPath(path_dropout)\n",
        "\n",
        "        self.conv = LocalFeedForward3D(out_channels, mlp_ratio)         # <-- LFFN3D class defined earlier in LTB Block\n",
        "\n",
        "        self.norm = norm_layer(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x + self.attention_path_dropout(self.mhca(x))\n",
        "        out = self.norm(x)  # Batchnormalization as done in original architecture\n",
        "        x = x + self.conv(out)\n",
        "        return x\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "oRK9F0JTJdEX"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}